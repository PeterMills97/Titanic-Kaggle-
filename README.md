# Titanic-Kaggle-
My notebook and submission for the Titanic Kaggle Completion 

I attempted this competion using the XGBoost learning algorithm. I selected XGBoost because a decison tree algorithm seemed the most appropriate solution to a classiffication problem with multiple catagorical variables. Without tuning the hyper parameters, the model initially overfit but I was able to tune to improve my score to a more respectable leaderboard postion. I experimented with feature engineering, using suggestions on the competioning forum although this did not yeild and improvement to my model. 
